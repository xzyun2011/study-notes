### 机器学习-风险函数, 损失函数, 经验函数, 贝叶斯决策

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200107224356884.png" alt="image-20200107224356884" style="zoom:60%;" />

**损失函数 loss function** ：

*L*(*Y*,*f*(*X*))， 度量预测错误的程度

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200107225133711.png" alt="image-20200107225133711" style="zoom:80%;" />



**风险函数（risk function）**：

损失函数的期望   $R_{e x p}(f)=E_{P}[L(Y, f(X))]=\int_{x * y} L(y, f(x)) P(x, y) d x d y$

学习的目标就是选择期望风险最小的模型。由于联合分布P(X,Y)是未知的，Rexp(f)不能直接计算。实际上，如果知道联合分布P(X,Y)，可以从联合分布直接求出条件概率分布P(Y|X)，也就不需要学习了。正因为不知道联合概率分布，所以才需要进行学习。这样一来，一方面根据期望风险最小学习模型要用的联合分布，另一方面联合分布又是未知的，所以监督学习就称为一个病态问题（ill-formed problem）

**经验风险（empirical risk）或经验损失（empirical loss）**:

给定一个训练集，模型*f*(*X*)f(X)关于训练数据集的平均损失  $R_{e m p}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200109134101698.png" alt="image-20200109134101698" style="zoom:60%;" />

训练过程中的误差，就是**训练误差**。

在验证集上进行交叉验证选择参数（调参），最终模型在验证集上的误差就是**验证误差**。

训练完毕、调参完毕的模型，在新的测试集上的误差，就是**测试误差**。

假如所有的数据来自一个整体，模型在这个整体上的误差，就是**泛化误差**。通常说来，测试误差的平均值或者说期望就是泛化误差。

综合来说，它们的大小关系为 训练误差 < 验证误差 < 测试误差 ～= 泛化误差

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200109134152141.png" alt="image-20200109134152141" style="zoom:60%;" />

**泛化误差（Generalization error）**：真实情况下模型的误差，**细分为Bias和Variance两个部分**



<img src="https://pic3.zhimg.com/v2-286539c808d9a429e69fd59fe33a16dd_r.jpg" alt="preview" style="zoom:40%;" />

准：bias描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。low bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。 确：varience描述的是样本上训练出来的模型在测试集上的表现，要想在variance上表现好，low varience，就要简化模型，减少模型的参数，但这样容易欠拟合(unfitting)，欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。

##### 2. 贝叶斯风险

无偏估计：

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200108230900554.png" alt="image-20200108230900554" style="zoom:80%;" />

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200108230935257.png" alt="image-20200108230935257" style="zoom:80%;" />

![image-20200108235805035](https://github.com/xzyun2011/study-notes/blob/main/img/image-20200108235805035.png)

MLE(极大似然估计)、MAP(最大后验估计)以及贝叶斯估计(Bayesian)**：MLE不考虑先验（prior)，MAP和贝叶斯估计则考虑先验(prior)；MLE、MAP是选择相对最好的一个模型（point estimation）， 贝叶斯方法则是通过观测数据来估计后验分布(posterior distribution)；当样本个数无穷多的时候，MAP理论上会逼近MLE.

<img src="https://pic1.zhimg.com/v2-321e2cb34e3fcde5c375a1fe24dffc64_r.jpg" alt="preview" style="zoom:60%;" />

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200108230232456.png" alt="image-20200108230232456" style="zoom:60%;" />

###### 先验概率**prior probability**： 指的是在观测前我们就已知的结果概率分布 p(y)

###### **后验概率posterior probability：指的是在观测到X后我们对结果y的估计**,比如p(y=1|X=x)

###### 贝叶斯公式：$p(A | B)=\frac{p(A \& B)}{P(B)}$  $p(A | B)=\frac{p(B | A) \cdot p(A)}{p(B)}$ 后验概率变得可计算

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200108221814114.png" alt="image-20200108221814114" style="zoom:80%;" />

数学解释：

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200109165519649.png" alt="image-20200109165519649" style="zoom:80%;" />

训练误差与测试误差，过拟合

<img src="https://github.com/xzyun2011/study-notes/blob/main/img/image-20200109165836526.png" alt="image-20200109165836526" style="zoom:80%;" />