### 				深度学习中的信息理论

##### **1. 信息熵（香农熵）Shannon entropy****

在信息论中，**熵**（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为**信息熵**、**信源熵**、**平均自信息量**。information? A measure of surprise. A measure of uncertainty.

信息熵不仅定量衡量了信息的大小，同时为信息编码提供了理论上的最优值：实用的编码平均码长的理论下界就是信息熵。即信息熵为数据压缩的极限。

**香农熵 Shannon entropy** 

离散型：$\mathrm{H}(X)=\sum_{i} \mathrm{P}\left(x_{i}\right) \mathrm{I}\left(x_{i}\right)=-\sum_{i} \mathrm{P}\left(x_{i}\right) \log _{b} \mathrm{P}\left(x_{i}\right)$

连续型：$H(X)=H(f)=-\int_{-\infty}^{+\infty} f(x) \log _{2} f(x) d x$

* 熵只依赖于随机变量的分布,与随机变量取值无关；

* 定义0log0=0(因为可能出现某个取值概率为0的情况)；

* 熵越大,随机变量的不确定性就越大,分布越混乱，随机变量状态数越多。它是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望**，**熵越大信息量越大（surprise越大）
* 当随机分布为均匀分布时，熵最大为$log_{b}^{n}$

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120195407256.png" alt="image-20200120195407256" style="zoom: 50%;" />

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120195505915.png" alt="image-20200120195505915" style="zoom: 50%;" />![image-20200120200907004](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120200907004.png)

##### 2. 联合信息熵和条件熵 Joint entropy and conditional entropy

**联合信息熵**：$H(X, Y)=-\sum_{i=1}^{n} \sum_{j=1}^{m} p\left(x_{i}, y_{j}\right) \log _{b} p\left(x_{i}, y_{j}\right)$

**条件熵**：$H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)$  	( X,Y 独立则为H(Y) )

**链式规则**：$H(X, Y)=H(X)+H(Y | X)=H(Y)+H(X | Y)$

**推论**1：$H(X, Y | Z)=H(X | Z)+H(Y | X, Z)$  (证明类似如下)

**推论**2：$H(X, Y, Z)=H(X)+H(Y, Z | X)=H(X)+H(Y | X)+H(Z | X, Y)$

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120202022868.png" alt="image-20200120202022868" style="zoom:50%;" />

##### **3. 相对熵Relative entropy （KL散度 Kullback–Leibler distance） **

定义： $D(p \| q)=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}=H(p,q)-H(p)\ge 0$ **（仅p=q 取0）**

**在贝叶斯推理中, 衡量当你修改了从先验分布 q 到后验分布 p 的信念之后带来的信息增益.  KL散度越小，真实分布与近视分布之间的匹配程度就越好**

相对熵较交叉熵有更多的优异性质，主要为：

* 当p分布和q分布相等时候，KL散度值为0，这是一个非常好的性质；

* 可以证明是非负的；

* 非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0。

  **在最优化问题中，最小化相对熵等价于最小化交叉熵**。交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小，相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异

  <img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120205626818.png" alt="image-20200120205626818" style="zoom:50%;" />

**4. 互信息 Mutual Information** **（对称性）**

互信息的定义为：**一个随机变量由于已知另一个随机变量而减少的不确定性**，或者说从贝叶斯角度考虑，由于新的观测数据y到来而导致x分布的不确定性下降程度。$I(X ; X)=H(X)$

$I(X ; Y)=\sum_{x \in X} \sum_{y \in Y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}=D(p(x, y) \| p(x) p(y)) \ge 0$    **(仅 x,y 独立取0)**
$$
\begin{aligned} I(X, Y) &=H(X)-H(X | Y)=H(Y)-H(Y | X) \\ &=H(X)+H(Y)-H(X, Y) \\ &=H(X, Y)-H(X | Y)-H(Y | X) \end{aligned} (symmetry)
$$
**The mutual information 𝐼(𝑋; 𝑌) is the reduction in the uncertainty of 𝑋 due to the knowledge of 𝑌:** $I(X, Y) =H(X)-H(X |Y) $

**条件互信息conditional mutual information**：$I(X, Y | Z)=H(X | Z)-H(X | Y, Z)$

**链式推理**：$I(X ; Y ; Z)=I(X ; Z)+I(Y ; Z | X)$

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120204735359.png" alt="image-20200120204735359" style="zoom:50%;" />Relations with Venn Diagram:

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120205726979.png" alt="image-20200120205726979" style="zoom:50%;" />

映射不变性 Reparametrization Invariance：

![image-20200120210208437](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120210208437.png)

互信息应用例子：<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120210226211.png" alt="image-20200120210226211" style="zoom:50%;" />

##### 5. 交叉熵 Cross-entropy

$$
L(p, q)=-\sum_{u \in\{0,1\}} p(u) \log _{2} q(u)=H(p)+D(p \| q)
$$

主要用于度量两个概率分布间的差异性信息，p是真实样本分布，q是预测得到样本分布
逻辑回归算法的损失函数就是交叉熵 https://zhuanlan.zhihu.com/p/35423404

##### **6. 数据处理不等性Data processing inequality**

马尔科夫链简单表示：

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120211504444.png" alt="image-20200120211504444" style="zoom:50%;" />![image-20200120211837727](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120211837727.png)

![image-20200120211837727](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120211837727.png)

**数据处理不等性Data processing inequality**：**信息在链中的传递不会增加，只会减少**

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120212203808.png" alt="image-20200120212203808" style="zoom:50%;" />![image-20200120212337824](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120212337824.png)

![image-20200120212337824](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120212337824.png)

##### 7. DNN 看成马尔科夫链

前馈神经网络：

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120212510982.png" alt="image-20200120212510982" style="zoom:50%;" />![image-20200120212825498](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120212825498.png)

<img src="C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120213741874.png" alt="image-20200120213741874" style="zoom:50%;" />![image-20200120213756178](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120213756178.png)

![image-20200120213756178](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120213756178.png)

信息瓶颈理论information bottleneck：https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.00810

https://zhuanlan.zhihu.com/p/29723280

![image-20200120214807569](C:\Users\xzyun2011\AppData\Roaming\Typora\typora-user-images\image-20200120214807569.png)